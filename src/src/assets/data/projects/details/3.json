{
    "id": 3,
    "image": {
        "name": "Indoor Localization Scene Classifier",
        "data": "../assets/images/proj.jpg"
    },
    "buttons": [
        {
            "type": "code",
            "link": "https://github.com/alexandertsema/IndoorLocalizationSceneClassifier"
        },
        {
            "type": "book",
            "link": "https://www.elsevier.com/books/computer-vision-for-assistive-healthcare/marco/978-0-12-813445-0"
        }
    ],
    "title": "Indoor Localization Scene Classifier",
    "subtitle": "Python, Tensorflow, NumPy",
    "text": "<p>This project is a part of fully functioning vision based assistive indoor localization system for visually impaired people. The convolutional neural network was developed to classify high-resolution omnidirectional images to reduce the time complexity of the sample search algorithm. Firstly, the dataset was generated and normalized. Secondly, the CNN’s architecture was developed and global parameters were optimized. Thirdly, the trained model was validated and tested on distinct datasets to ensure generalization ability correctness. The classification accuracy for top 1 candidate class on the testing dataset is up to 89% on average, for top 2 candidate classes is 100%. All work was done using Tensorflow 1.0 with original Python API. All the insides of the CNN were visualized in the Tensorboard. The results of the research under the lead of Professor Zhigang Zhu will be published in a book “Computer Vision for Assistive Healthcare” by April 2018.</p><p>An indoor localization system is of significant importance to the visually impaired in their daily lives if it can help them localize themselves and further navigate unfamiliar indoor environments. There are 285 million visually impaired people in the world according to the World Health Organization, among whom 39 million are blind. Compared to sighted people, it is much harder for visually impaired people to navigate indoor environments. Nowadays, too many buildings are also unfortunately mainly designed and built for sighted people; therefore, navigational tasks and functions that sighted people take for granted could be huge problems to visually impaired people.</p><p>The current trained convolutional neural network is not optimal and it will take many hours to find the optimal architecture and global parameters if we only use CPUs. To speed up things we will train our model on GPU cluster in some cloud like AWS, Azure or Google Cloud Platform. Second, data collection and manual labeling is still tedious and time consuming, so we will explore efficient strategy to automate this process. Another important thing is that we work with video stream, which means that captured frames have one crucial fact one more dimension time. This means that each image depends on previous images and influences on the following ones. Thus, for the time series data it is common to use Recurrent Neural Networks, and we will try to combine RNN with our existing approach. Using temporal information also applies to the following steps.</p>"
}